{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring President Trump's Daily Task Force Briefings with Emotion Analysis and Topic Modelling\n",
    "\n",
    "![](../imgs/trump_task_force.jpg)\n",
    "\n",
    "Given the controversy over the Trump administration's [daily briefings](https://www.nytimes.com/2020/04/09/us/politics/trump-coronavirus-press-briefing.html?referringSource=articleShare) and overall [handling of the COVID-19 pandemic](https://www.nytimes.com/2020/04/14/us/politics/coronavirus-trump-who-funding.html), I thought it would be interesting to apply some of the basic NLP techniques I recently learned to explore this timely, readily available body of text. The analysis below examines all of the White House's task force briefings held from 2020-02-26 to 2020-04-27."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Briefing Transcripts\n",
    "Although the briefing transcripts are available on [rev.com](https://www.rev.com) in formatted `.txt` files, I wanted to practice using [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) for web scraping as part of this project. The scraped transcripts are included [here](https://github.com/brendoncampbell/corona-briefing-tones/blob/master/data/all_briefings.csv), and [`scrape_briefings.py`](https://github.com/brendoncampbell/corona-briefing-tones/blob/master/src/scrape_briefings.py) can be run to rescrape them.\n",
    "\n",
    "After reading in this relatively clean CSV, we have a simple dataframe containing chronological paragraphs of speech `text` along with the corresponding `date`, `timestamp` and `speaker`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RendererRegistry.enable('png')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt.renderers.enable('png')\n",
    "#alt.renderers.enable('mimetype')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>speaker</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>05:39</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Thank you very much everybody. Thank you very ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>06:59</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>A lot of people thought we shouldn’t have done...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>07:51</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>We have a total of 15. We took in some from Ja...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>09:58</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>China you know about. Where it started. I spok...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>10:52</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>We’re bringing in a specialist, a very highly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9677</th>\n",
       "      <td>2020-04-27</td>\n",
       "      <td>01:01:23</td>\n",
       "      <td>Reporters</td>\n",
       "      <td>[crosstalk 00:13:23].</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9678</th>\n",
       "      <td>2020-04-27</td>\n",
       "      <td>01:01:24</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Let’s do one more. Please, in the back.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9679</th>\n",
       "      <td>2020-04-27</td>\n",
       "      <td>01:01:26</td>\n",
       "      <td>Speaker 16</td>\n",
       "      <td>If an American president loses more Americans ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9680</th>\n",
       "      <td>2020-04-27</td>\n",
       "      <td>01:01:36</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>So yeah, we’ve lost a lot of people, but if yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9681</th>\n",
       "      <td>2020-04-27</td>\n",
       "      <td>01:02:17</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>The press doesn’t talk about ventilators anymo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9682 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date timestamp       speaker  \\\n",
       "0     2020-02-26     05:39  Donald Trump   \n",
       "1     2020-02-26     06:59  Donald Trump   \n",
       "2     2020-02-26     07:51  Donald Trump   \n",
       "3     2020-02-26     09:58  Donald Trump   \n",
       "4     2020-02-26     10:52  Donald Trump   \n",
       "...          ...       ...           ...   \n",
       "9677  2020-04-27  01:01:23     Reporters   \n",
       "9678  2020-04-27  01:01:24  Donald Trump   \n",
       "9679  2020-04-27  01:01:26    Speaker 16   \n",
       "9680  2020-04-27  01:01:36  Donald Trump   \n",
       "9681  2020-04-27  01:02:17  Donald Trump   \n",
       "\n",
       "                                                   text  \n",
       "0     Thank you very much everybody. Thank you very ...  \n",
       "1     A lot of people thought we shouldn’t have done...  \n",
       "2     We have a total of 15. We took in some from Ja...  \n",
       "3     China you know about. Where it started. I spok...  \n",
       "4     We’re bringing in a specialist, a very highly ...  \n",
       "...                                                 ...  \n",
       "9677                              [crosstalk 00:13:23].  \n",
       "9678            Let’s do one more. Please, in the back.  \n",
       "9679  If an American president loses more Americans ...  \n",
       "9680  So yeah, we’ve lost a lot of people, but if yo...  \n",
       "9681  The press doesn’t talk about ventilators anymo...  \n",
       "\n",
       "[9682 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import scraped csv to pandas df\n",
    "briefings_df = pd.read_csv('../data/all_briefings.csv')\n",
    "briefings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null Values\n",
    "Checking for null values, we can see there are actually only three missing `text` values in the near 10,000 rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>speaker</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7217</th>\n",
       "      <td>2020-04-13</td>\n",
       "      <td>16:03</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8339</th>\n",
       "      <td>2020-04-19</td>\n",
       "      <td>23:03</td>\n",
       "      <td>Andrew Cuomo</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9181</th>\n",
       "      <td>2020-04-22</td>\n",
       "      <td>01:15:03</td>\n",
       "      <td>Dr. Birx</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date timestamp       speaker text\n",
       "7217  2020-04-13     16:03  Donald Trump  NaN\n",
       "8339  2020-04-19     23:03  Andrew Cuomo  NaN\n",
       "9181  2020-04-22  01:15:03      Dr. Birx  NaN"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "briefings_df[briefings_df['text'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick look at the full transcripts and video recordings confirms these correspond to the speaker interjecting, being cut off, or uttering something inaudible. For the purpose of this analysis, let's simply drop these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "briefings_df = briefings_df.dropna(subset=['text']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Up Speaker Names\n",
    "\n",
    "Checking the names of the most frequent speakers, we see a few opportunities to clean up this column before moving on to preprocessing.\n",
    "\n",
    "1. Unnamed speakers identified by a number (i.e. `Speaker 12`) are reset for each briefing, and therefore don't map to the same person.\n",
    "2. President Trump and some of the other key task force members are referenced by multiple names (`Dr. Birx`, `Dr. Deborah Birx`).\n",
    "3. There are a handful of male reporters (John, Jeff, Jim, Peter, and Steve) frequently called upon by their first name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Donald Trump              4013\n",
       "Mike Pence                 751\n",
       "Dr. Birx                   244\n",
       "Dr. Fauci                  237\n",
       "John                       161\n",
       "Deborah Birx               101\n",
       "Admiral Giroir              94\n",
       "Dr. Deborah Birx            94\n",
       "Speaker 8                   93\n",
       "Speaker 7                   92\n",
       "Speaker 5                   88\n",
       "Steve Mnuchin               84\n",
       "Speaker 12                  82\n",
       "Speaker 11                  82\n",
       "Speaker 22                  79\n",
       "Speaker 3                   78\n",
       "Speaker 9                   76\n",
       "Speaker 10                  76\n",
       "Speaker 6                   73\n",
       "Speaker 4                   73\n",
       "Speaker 13                  72\n",
       "Speaker 14                  68\n",
       "Speaker 2                   65\n",
       "Speaker 19                  64\n",
       "Speaker 15                  61\n",
       "Speaker 16                  60\n",
       "Mike Pompeo                 59\n",
       "Reporter                    58\n",
       "Jeff                        56\n",
       "Vice President Pence        53\n",
       "Speaker 23                  52\n",
       "Speaker 18                  52\n",
       "Speaker 1                   49\n",
       "Jim                         47\n",
       "Speaker 17                  47\n",
       "President Donald Trump      45\n",
       "Steve                       45\n",
       "Speaker 20                  44\n",
       "Seema Verma                 42\n",
       "Dr. Hahn                    42\n",
       "Name: speaker, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many paragraphs of text for the top speakers?\n",
    "briefings_df['speaker'].value_counts()[:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consolidate these so we have consistent speaker names and group the unnamed speakers under a single name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Donald Trump          4091\n",
       "Unnamed               2038\n",
       "Mike Pence             849\n",
       "Dr. Deborah Birx       491\n",
       "Dr. Anthony Fauci      317\n",
       "Unnamed (Reporter)     215\n",
       "John (Reporter)        162\n",
       "Steve Mnuchin          136\n",
       "Brett Giroir           100\n",
       "Mike Pompeo             81\n",
       "Dr. Steven Hahn         64\n",
       "Alex Azar               63\n",
       "Jeff (Reporter)         56\n",
       "Seema Verma             56\n",
       "John Polowczyk          49\n",
       "Jerome Adams            48\n",
       "Jim (Reporter)          47\n",
       "Steve (Reporter)        46\n",
       "Peter Navarro           35\n",
       "Peter (Reporter)        33\n",
       "Name: speaker, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace speaker names using basic regex\n",
    "briefings_df['speaker'].replace(regex={r'.*Trump.*': 'Donald Trump', \n",
    "                                       r'.*Pence.*': 'Mike Pence',\n",
    "                                       r'.*Fauci.*': 'Dr. Anthony Fauci',\n",
    "                                       r'.*Birx.*': 'Dr. Deborah Birx',\n",
    "                                       r'.*Berks.*': 'Dr. Deborah Birx',\n",
    "                                       r'.*Pompeo.*': 'Mike Pompeo',\n",
    "                                       r'.*Report.*': 'Unnamed (Reporter)',\n",
    "                                       r'.*Audience Member.*': 'Unnamed',\n",
    "                                       r'.*Speaker .*': 'Unnamed', \n",
    "                                       r'.*Jeff\\Z': 'Jeff (Reporter)',\n",
    "                                       r'.*John\\Z': 'John (Reporter)',\n",
    "                                       r'.*Peter\\Z': 'Peter (Reporter)',\n",
    "                                       r'.*Jim\\Z': 'Jim (Reporter)',\n",
    "                                       r'.*Steve\\Z': 'Steve (Reporter)',\n",
    "                                       r'.*Pete\\Z': 'Pete Gaynor',\n",
    "                                       r'.*Novarro.*': 'Peter Navarro',\n",
    "                                       r'.*Surgeon General.*': 'Jerome Adams',\n",
    "                                       r'.*Giroir.*': 'Brett Giroir',\n",
    "                                       r'.*Polowczyk.*': 'John Polowczyk',\n",
    "                                       r'.*Verma.*': 'Seema Verma',\n",
    "                                       r'.*Azar.*': 'Alex Azar',\n",
    "                                       r'.*Hahn.*': 'Dr. Steven Hahn',\n",
    "                                       r'.*Mnuchin.*': 'Steve Mnuchin'},\n",
    "                                inplace = True)\n",
    "\n",
    "briefings_df['speaker'].value_counts()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language is inherently unstructured compared to most types of data, so we need to preprocess the text before moving onto analysis. We will carry out this preprocessing and normalize things somewhat using the [spaCy](https://spacy.io) package:\n",
    "\n",
    "1. Convert text to lower-case\n",
    "2. Tokenize (identify word boundaries and split text)\n",
    "3. Remove stop words (frequently-occurring English words that don't tend to be useful for analysis)\n",
    "4. Lemmatize (convert words to their base form: spreading → spread)\n",
    "5. Exclude irrelevant tokens such as emails, URLS and unimportant parts of speech (i.e. pronouns, conjunctions, punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from pandarallel import pandarallel\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English spaCy model and stop words\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# function for preprocessing each paragraph of transcript text\n",
    "def preprocess(text, \n",
    "               min_token_len = 2, \n",
    "               irrelevant_pos = ['ADV','PRON','CCONJ','PUNCT','PART','DET','ADP','SPACE']): \n",
    "    \"\"\"\n",
    "    Carry out preprocessing of the text and return a preprocessed list of strings. \n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    text : (str) \n",
    "        the text to be preprocessed\n",
    "    min_token_len : (int) \n",
    "        min_token_length required\n",
    "    irrelevant_pos : (list) \n",
    "        a list of irrelevant pos tags\n",
    "    \n",
    "    Returns\n",
    "    -------------\n",
    "    (list) the preprocessed text as a list of strings\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert input string to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove multiple whitespace characters\n",
    "    text = re.sub(r'\\s+',' ', text)\n",
    "    \n",
    "    # tokenize with spacy, exluding stop words, short tokens, \n",
    "    # irrelevant POS, emails, urls, and strings containing \n",
    "    # non-alphanumeric chars\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    token_list = []\n",
    "    for token in doc:\n",
    "        if token.is_stop == False and len(token.text)>=min_token_len \\\n",
    "            and token.pos_ not in irrelevant_pos and token.like_email == False \\\n",
    "            and token.like_url == False and token.text.isalnum():\n",
    "            token_list.append(token.lemma_)\n",
    "        \n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the `preprocess()` function defined above to each briefing text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [thank, thank, begin, like, extend, deep, cond...\n",
       "1       [lot, people, think, turn, good, thing, number...\n",
       "2       [total, 15, take, japan, hear, american, citiz...\n",
       "3       [china, know, start, speak, president, xi, gre...\n",
       "4       [bring, specialist, regarded, specialist, tomo...\n",
       "                              ...                        \n",
       "9674                                          [crosstalk]\n",
       "9675                                                [let]\n",
       "9676    [american, president, lose, americans, course,...\n",
       "9677    [yeah, lose, lot, people, look, original, proj...\n",
       "9678    [press, talk, ventilator, want, talk, okay, re...\n",
       "Name: pp_text, Length: 9679, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parallelize and apply preprocessor to each text\n",
    "pandarallel.initialize(verbose=False)\n",
    "briefings_df['pp_text'] = briefings_df.text.parallel_apply(preprocess)\n",
    "briefings_df['pp_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotion and Sentiment Analysis\n",
    "\n",
    "Now that we have a nice clean dataframe to work with, it's time to move on to analysis. Rather than applying the popular [TextBlob](https://textblob.readthedocs.io/en/dev/) or [Vader](https://github.com/cjhutto/vaderSentiment) packages commonly used for sentiment analysis, I thought it would be interesting to also explore the emotional tone of briefing texts.\n",
    "\n",
    "Let's see what we can uncover using the NRC Word-Emotion Association Lexicon, [EmoLex](https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm). In addition to 'positive' and 'negative', we have word associations for eight basic emotion categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>index</th>\n",
       "      <th>word</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abacus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abandon</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abandoned</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abandonment</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abba</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6437</th>\n",
       "      <td>zany</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6438</th>\n",
       "      <td>zeal</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6439</th>\n",
       "      <td>zealous</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6440</th>\n",
       "      <td>zest</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6441</th>\n",
       "      <td>zip</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6442 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "index         word  anger  anticipation  disgust  fear  joy  negative  \\\n",
       "0           abacus      0             0        0     0    0         0   \n",
       "1          abandon      0             0        0     1    0         1   \n",
       "2        abandoned      1             0        0     1    0         1   \n",
       "3      abandonment      1             0        0     1    0         1   \n",
       "4             abba      0             0        0     0    0         0   \n",
       "...            ...    ...           ...      ...   ...  ...       ...   \n",
       "6437          zany      0             0        0     0    0         0   \n",
       "6438          zeal      0             1        0     0    1         0   \n",
       "6439       zealous      0             0        0     0    1         0   \n",
       "6440          zest      0             1        0     0    1         0   \n",
       "6441           zip      0             0        0     0    0         1   \n",
       "\n",
       "index  positive  sadness  surprise  trust  \n",
       "0             0        0         0      1  \n",
       "1             0        1         0      0  \n",
       "2             0        1         0      0  \n",
       "3             0        1         1      0  \n",
       "4             1        0         0      0  \n",
       "...         ...      ...       ...    ...  \n",
       "6437          0        0         1      0  \n",
       "6438          1        0         1      1  \n",
       "6439          1        0         0      1  \n",
       "6440          1        0         0      1  \n",
       "6441          0        0         0      0  \n",
       "\n",
       "[6442 rows x 11 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in raw emotion lexicon\n",
    "filepath = \"../NRC-Sentiment-Emotion-Lexicons/NRC-Emotion-Lexicon-v0.92/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\"\n",
    "emolex_df = pd.read_csv(filepath,  names=[\"word\", \"emotion\", \"association\"], skiprows=1, sep='\\t')\n",
    "\n",
    "# pivot df so we have one row per word, one column per emotion\n",
    "emolex_df = emolex_df.pivot(index='word', columns='emotion', values='association').reset_index()\n",
    "emolex_df.columns.name = 'index'\n",
    "\n",
    "# filter out words without scores, as well as those with more than 7 scores\n",
    "emolex_df = emolex_df[emolex_df.sum(axis=1)>0].reset_index(drop=True)\n",
    "emolex_df = emolex_df[emolex_df.sum(axis=1)<7].reset_index(drop=True)\n",
    "emolex_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this lexicon to easily retrieve associations for the words in a single briefing `text`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you, Mr. President. We just completed today’s meeting of the White House coronavirus task force. We had the opportunity to brief the President today on a broad range of issues. Once again, because of the unprecedented action that President Trump took in January, suspending all travel from China, establishing travel advisories for portions of South Korea and Italy, establishing screening of all direct flights, all passengers from all airports, Italy and South Korea… we have bought a considerable amount of time, according to all the health experts, to deal with the coronavirus here in the United States.\n"
     ]
    }
   ],
   "source": [
    "paragraph_text = briefings_df.text[504]\n",
    "print(paragraph_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thank', 'mr', 'president', 'complete', 'today', 'meeting', 'white', 'house', 'coronavirus', 'task', 'force', 'opportunity', 'brief', 'president', 'today', 'broad', 'range', 'issue', 'unprecedented', 'action', 'president', 'trump', 'take', 'january', 'suspend', 'travel', 'china', 'establish', 'travel', 'advisory', 'portion', 'south', 'korea', 'italy', 'establish', 'screening', 'direct', 'flight', 'passenger', 'airport', 'italy', 'south', 'korea', 'buy', 'considerable', 'time', 'accord', 'health', 'expert', 'deal', 'coronavirus', 'united', 'states']\n"
     ]
    }
   ],
   "source": [
    "paragraph_tokens = briefings_df.pp_text[504]\n",
    "print(paragraph_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>index</th>\n",
       "      <th>word</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>accord</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>action</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>airport</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>considerable</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1460</th>\n",
       "      <td>deal</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2098</th>\n",
       "      <td>establish</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184</th>\n",
       "      <td>expert</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2392</th>\n",
       "      <td>force</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4088</th>\n",
       "      <td>opportunity</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4209</th>\n",
       "      <td>passenger</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4485</th>\n",
       "      <td>president</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5736</th>\n",
       "      <td>task</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5831</th>\n",
       "      <td>time</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5933</th>\n",
       "      <td>trump</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6073</th>\n",
       "      <td>united</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6091</th>\n",
       "      <td>unprecedented</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6345</th>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "index           word  anger  anticipation  disgust  fear  joy  negative  \\\n",
       "55            accord      0             0        0     0    0         0   \n",
       "80            action      0             0        0     0    0         0   \n",
       "174          airport      0             1        0     0    0         0   \n",
       "1182    considerable      0             0        0     0    0         0   \n",
       "1460            deal      0             1        0     0    1         0   \n",
       "2098       establish      0             0        0     0    0         0   \n",
       "2184          expert      0             0        0     0    0         0   \n",
       "2392           force      1             0        0     1    0         1   \n",
       "4088     opportunity      0             1        0     0    0         0   \n",
       "4209       passenger      0             1        0     0    0         0   \n",
       "4485       president      0             0        0     0    0         0   \n",
       "5736            task      0             0        0     0    0         0   \n",
       "5831            time      0             1        0     0    0         0   \n",
       "5933           trump      0             0        0     0    0         0   \n",
       "6073          united      0             0        0     0    0         0   \n",
       "6091   unprecedented      0             0        0     0    0         0   \n",
       "6345           white      0             1        0     0    1         0   \n",
       "\n",
       "index  positive  sadness  surprise  trust  \n",
       "55            1        0         0      1  \n",
       "80            1        0         0      0  \n",
       "174           0        0         0      0  \n",
       "1182          1        0         0      0  \n",
       "1460          1        0         1      1  \n",
       "2098          0        0         0      1  \n",
       "2184          1        0         0      1  \n",
       "2392          0        0         0      0  \n",
       "4088          1        0         0      0  \n",
       "4209          0        0         0      0  \n",
       "4485          1        0         0      1  \n",
       "5736          1        0         0      0  \n",
       "5831          0        0         0      0  \n",
       "5933          0        0         1      0  \n",
       "6073          1        0         0      1  \n",
       "6091          0        0         1      0  \n",
       "6345          1        0         0      1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emolex_df[pd.DataFrame(emolex_df.word.tolist()).isin(paragraph_tokens).any(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following this approach, let's calculate and store aggregate emotion scores for each briefing paragraph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>speaker</th>\n",
       "      <th>text</th>\n",
       "      <th>pp_text</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>05:39</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Thank you very much everybody. Thank you very ...</td>\n",
       "      <td>[thank, thank, begin, like, extend, deep, cond...</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>06:59</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>A lot of people thought we shouldn’t have done...</td>\n",
       "      <td>[lot, people, think, turn, good, thing, number...</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>07:51</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>We have a total of 15. We took in some from Ja...</td>\n",
       "      <td>[total, 15, take, japan, hear, american, citiz...</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>09:58</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>China you know about. Where it started. I spok...</td>\n",
       "      <td>[china, know, start, speak, president, xi, gre...</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>10:52</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>We’re bringing in a specialist, a very highly ...</td>\n",
       "      <td>[bring, specialist, regarded, specialist, tomo...</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date timestamp       speaker  \\\n",
       "0  2020-02-26     05:39  Donald Trump   \n",
       "1  2020-02-26     06:59  Donald Trump   \n",
       "2  2020-02-26     07:51  Donald Trump   \n",
       "3  2020-02-26     09:58  Donald Trump   \n",
       "4  2020-02-26     10:52  Donald Trump   \n",
       "\n",
       "                                                text  \\\n",
       "0  Thank you very much everybody. Thank you very ...   \n",
       "1  A lot of people thought we shouldn’t have done...   \n",
       "2  We have a total of 15. We took in some from Ja...   \n",
       "3  China you know about. Where it started. I spok...   \n",
       "4  We’re bringing in a specialist, a very highly ...   \n",
       "\n",
       "                                             pp_text     anger  anticipation  \\\n",
       "0  [thank, thank, begin, like, extend, deep, cond...  0.428571      0.071429   \n",
       "1  [lot, people, think, turn, good, thing, number...  0.111111      0.444444   \n",
       "2  [total, 15, take, japan, hear, american, citiz...  0.187500      0.312500   \n",
       "3  [china, know, start, speak, president, xi, gre...  0.083333      0.250000   \n",
       "4  [bring, specialist, regarded, specialist, tomo...  0.272727      0.090909   \n",
       "\n",
       "    disgust      fear       joy  negative  positive   sadness  surprise  \\\n",
       "0  0.214286  0.428571  0.142857  0.571429  0.285714  0.428571  0.142857   \n",
       "1  0.111111  0.333333  0.111111  0.333333  0.444444  0.222222  0.111111   \n",
       "2  0.062500  0.250000  0.187500  0.250000  0.437500  0.250000  0.250000   \n",
       "3  0.000000  0.083333  0.250000  0.250000  0.666667  0.000000  0.083333   \n",
       "4  0.181818  0.545455  0.000000  0.636364  0.181818  0.454545  0.272727   \n",
       "\n",
       "      trust  \n",
       "0  0.071429  \n",
       "1  0.333333  \n",
       "2  0.187500  \n",
       "3  0.500000  \n",
       "4  0.090909  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create empty df to store aggregated emotion calcs\n",
    "data = pd.DataFrame([])\n",
    "\n",
    "for tokens in briefings_df['pp_text']:\n",
    "    paragraph_emos = emolex_df[pd.DataFrame(emolex_df.word.tolist()).isin(tokens).any(1)].mean()\n",
    "    data = data.append(paragraph_emos, ignore_index=True)\n",
    "    \n",
    "# combine aggregated emotion scores with transcript df\n",
    "briefings_df = briefings_df.join(data)\n",
    "\n",
    "# drop empty 'word' column, fill NaNs with zero\n",
    "briefings_df = briefings_df.drop(columns=['word'])\n",
    "briefings_df = briefings_df.fillna(0)\n",
    "\n",
    "briefings_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modelling\n",
    "\n",
    "Now that we have emotion and sentiment scores, let's apply topic modelling to see if we can identify the major themes discussed during the briefings and classify each briefing `text` accordingly. We'll use the [gensim](https://pypi.org/project/gensim/) package to build a Latent Dirichlet Allocation (LDA) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim import models\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a dictionary of all of the word tokens we consider relevant for topic modelling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dictionary\n",
    "corpus = briefings_df['pp_text'].tolist()\n",
    "dictionary = corpora.Dictionary(corpus)\n",
    "\n",
    "# filter extremes, removing tokens that appear in either:\n",
    "# fewer than 10 texts, or in more than 10% of all tets\n",
    "dictionary.filter_extremes(no_below = 10, no_above = 0.1)\n",
    "\n",
    "# define words to be manually removed and retrieve their indexes\n",
    "remove_words = ['crosstalk', 'question', 'inaudible', 'mr', 'sir', 'dr']\n",
    "del_indexes = [k for k,v in dictionary.items() if v in remove_words]\n",
    "\n",
    "# remove unwanted word ids from the dictionary\n",
    "dictionary.filter_tokens(bad_ids=del_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a [document-term co-occurrence matrix](https://en.wikipedia.org/wiki/Document-term_matrix), consisting of the bag-of-words (BoW) representation of each `text`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With both of these, we can build and visualize an LDA topic model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = models.LdaModel(corpus=doc_term_matrix,\n",
    "                            id2word=dictionary,\n",
    "                            num_topics=6,\n",
    "                            passes=20,\n",
    "                            random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = pyLDAvis.gensim.prepare(lda_model, doc_term_matrix, dictionary, sort_topics=False)\n",
    "\n",
    "# enable for interactive topic model visualization:\n",
    "#    pyLDAvis.enable_notebook()\n",
    "#    viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After inspecting the interactive output rendered by [pyLDAvis](https://github.com/bmabey/pyLDAvis), experimenting with different numbers of topics, and filtering some unhelpful and extreme tokens, we see logical results with the following six topics being identified:\n",
    "1. Economy\n",
    "2. International\n",
    "3. Policy & Guidelines\n",
    "4. Testing\n",
    "5. Ventilators & NY Outbreak\n",
    "6. Other\n",
    "\n",
    "![](../imgs/topics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the topic model to predict the topic for each individual `text`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_labels = {0:'Economy',\n",
    "                1:'International',\n",
    "                2:'Policy & Guidelines',\n",
    "                3:'Testing',\n",
    "                4:'Ventilators & NY Outbreak',\n",
    "                5:'Other'\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_prob_topic(unseen_document, model = lda_model):\n",
    "    \"\"\"\n",
    "    Given an unseen_document, and a trained LDA model, this function\n",
    "    finds the most likely topic (topic with the highest probability) from the \n",
    "    topic distribution of the unseen document and returns the best topic\n",
    "    \n",
    "    Parameters\n",
    "    ------------\n",
    "    unseen_text : (str) \n",
    "        the text to be labeled with a topic\n",
    "    model : (gensim ldamodel) \n",
    "        the trained LDA model\n",
    "    \n",
    "    Returns: \n",
    "    -------------\n",
    "        (str) the most likely topic label\n",
    "    \n",
    "    Examples:\n",
    "    ----------\n",
    "    >> get_most_prob_topic(\"We're building so so so many ventilators.\", \n",
    "                            model = lda)\n",
    "    Ventilators\n",
    "    \"\"\"\n",
    "    \n",
    "    # obtain bow vector for unseen text\n",
    "    bow_vector = dictionary.doc2bow(unseen_text)\n",
    "    \n",
    "    # calculate topic scores for unseen text\n",
    "    scores_df = pd.DataFrame(lda_model[bow_vector], columns =['topic', 'score']) \n",
    "    \n",
    "    # find topic name of max score\n",
    "    topic_name = topic_labels[scores_df.loc[scores_df['score'].idxmax(), 'topic']]\n",
    "    best_score = scores_df['score'].max()\n",
    "    \n",
    "    return topic_name, best_score;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pp_text</th>\n",
       "      <th>topic_pred</th>\n",
       "      <th>topic_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[thank, thank, begin, like, extend, deep, cond...</td>\n",
       "      <td>International</td>\n",
       "      <td>0.588161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[lot, people, think, turn, good, thing, number...</td>\n",
       "      <td>Policy &amp; Guidelines</td>\n",
       "      <td>0.499821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[total, 15, take, japan, hear, american, citiz...</td>\n",
       "      <td>International</td>\n",
       "      <td>0.377780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[china, know, start, speak, president, xi, gre...</td>\n",
       "      <td>International</td>\n",
       "      <td>0.603302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[bring, specialist, regarded, specialist, tomo...</td>\n",
       "      <td>Testing</td>\n",
       "      <td>0.306851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             pp_text           topic_pred  \\\n",
       "0  [thank, thank, begin, like, extend, deep, cond...        International   \n",
       "1  [lot, people, think, turn, good, thing, number...  Policy & Guidelines   \n",
       "2  [total, 15, take, japan, hear, american, citiz...        International   \n",
       "3  [china, know, start, speak, president, xi, gre...        International   \n",
       "4  [bring, specialist, regarded, specialist, tomo...              Testing   \n",
       "\n",
       "   topic_score  \n",
       "0     0.588161  \n",
       "1     0.499821  \n",
       "2     0.377780  \n",
       "3     0.603302  \n",
       "4     0.306851  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create empty lists to store prediction strings\n",
    "predictions = []\n",
    "scores = []\n",
    "\n",
    "# call function for each unseen text, appending predictions to list\n",
    "for text in briefings_df['pp_text'].tolist():\n",
    "    # only predict a topic for texts where there are 4 or more tokens\n",
    "    if len(text) > 4:\n",
    "        topic, value = get_most_prob_topic(text)\n",
    "        predictions.append(topic)\n",
    "        scores.append(value)\n",
    "    else:\n",
    "        predictions.append(np.nan)\n",
    "        scores.append(np.nan)\n",
    "\n",
    "# add prediction values to main df\n",
    "briefings_df['topic_pred'] = predictions\n",
    "briefings_df['topic_score'] = scores\n",
    "\n",
    "# save scored df to csv\n",
    "briefings_df.to_csv(\"../data/scored_briefings.csv\",index=False)\n",
    "\n",
    "briefings_df[['pp_text','topic_pred','topic_score']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draft Analysis and Visualization (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataframe of aggregate sentiment scores by date for altair plotting\n",
    "briefings_by_date = briefings_df.groupby(['date']).mean().reset_index()\n",
    "sentiment_by_date = briefings_by_date[['positive', 'negative', 'date']]\n",
    "sentiment_by_date = sentiment_by_date.melt(['date'], var_name='emo', value_name='score')\n",
    "\n",
    "# plot aggregated scores\n",
    "alt.Chart(sentiment_by_date).mark_line().encode(\n",
    "    alt.X('date:T', axis=alt.Axis(title='Briefing Date', labelAngle=0)),\n",
    "    alt.Y('score:Q', axis=alt.Axis(title='Sentiment Score')),\n",
    "    alt.Color('emo:N', \n",
    "              legend=alt.Legend(title='Sentiment'), \n",
    "              sort=['positive'])\n",
    ").properties(\n",
    "    title = 'Sentiment Scores Aggregated by Briefing',\n",
    "    height = 300,\n",
    "    width = 800\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataframe of aggregate emotion scores by date for altair plotting\n",
    "emotion_by_date = briefings_by_date.drop(columns=['topic_score', 'positive', 'negative'])\n",
    "emotion_by_date = emotion_by_date.melt(['date'], var_name='emo', value_name='score')\n",
    "\n",
    "# plot aggregated scores\n",
    "alt.Chart(emotion_by_date).mark_line().encode(\n",
    "    alt.X('date:T', axis=alt.Axis(title='Briefing Date', labelAngle=0)),\n",
    "    alt.Y('score:Q', axis=alt.Axis(title='Emotion Score')),\n",
    "    alt.Color('emo:N', \n",
    "              legend=alt.Legend(title='Emotion'),\n",
    "              sort=['trust','anticipation','fear','joy','sadness','anger','surprise','red'])\n",
    ").properties(    \n",
    "    title = 'Emotion Scores Aggregated by Briefing',\n",
    "    height = 400,\n",
    "    width = 800\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot heatmap by date for a speaker\n",
    "\n",
    "def emo_heatmap_by_date(speaker):\n",
    "    \"\"\"\n",
    "    Generates a heatmap of emotion scores per briefing\n",
    "    for the specied speaker.\n",
    "    \"\"\"\n",
    "    \n",
    "    speaker_feels = briefings_df[briefings_df['speaker']==speaker]\n",
    "    speaker_feels = speaker_feels.drop(columns=['topic_score', 'positive', 'negative'])\n",
    "    \n",
    "    speaker_feels_by_date = speaker_feels.groupby(['date']).mean().reset_index()\n",
    "    speaker_feels_by_date = speaker_feels_by_date.melt('date', var_name='emo', value_name='score')\n",
    "\n",
    "\n",
    "    plot = alt.Chart(speaker_feels_by_date).mark_rect().encode(\n",
    "        alt.X('date:O', axis=alt.Axis(title='Briefing Date')),\n",
    "        alt.Y('emo:N', axis=alt.Axis(title='Emotion')),\n",
    "        alt.Color('score:Q', \n",
    "                  legend=alt.Legend(title='Score'),\n",
    "                  scale=alt.Scale(scheme='lighttealblue'))\n",
    "    ).properties(title = \"Heatmap of Emotion Scores by Briefing for \"+speaker)\n",
    "    \n",
    "    return plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_heatmap_by_date('Donald Trump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_heatmap_by_date('Dr. Deborah Birx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_heatmap_by_date('Dr. Anthony Fauci')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_date_heatmap(emo):\n",
    "    \"\"\"\n",
    "    Generates a heatmap of topic predictions per briefing\n",
    "    for the specied emotion or sentiment.\n",
    "    \"\"\"\n",
    "    \n",
    "    topics_by_date = briefings_df.groupby(['topic_pred', 'date']).mean().reset_index()\n",
    "    topics_by_date = topics_by_date.melt(['topic_pred', 'date'], var_name='emo', value_name='score')\n",
    "    \n",
    "    plot = alt.Chart(topics_by_date[topics_by_date['emo'] == emo]).mark_rect().encode(  \n",
    "        alt.X('date:O', axis=alt.Axis(title='Briefing Date')),\n",
    "        alt.Y('topic_pred:N', axis=alt.Axis(title='Topic')),\n",
    "        alt.Color('score:Q', \n",
    "                  legend=alt.Legend(title=['Aggregated', emo.capitalize()+' Score']),\n",
    "                  scale=alt.Scale(scheme='lighttealblue'))\n",
    "    ).properties(title = 'Heatmap of ' + emo.capitalize() + ' Scores for each Topic by Briefing')\n",
    "    \n",
    "    return plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_date_heatmap('positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_date_heatmap('fear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_date_heatmap('anger')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
