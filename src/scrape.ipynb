{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape trump transcripts from https://www.rev.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_briefings_resp = requests.get(\"https://www.rev.com/blog/transcript-tag/trump-coronavirus-task-force-briefing-transcripts\")\n",
    "all_briefings = bs(all_briefings_resp.text)\n",
    "\n",
    "first_page_briefings = bs(all_briefings_resp.text)\n",
    "\n",
    "first_page_briefings_list = first_page_briefings.find_all(\"div\",{\"class\":\"fl-post-column\"})\n",
    "\n",
    "current_page_briefings = first_page_briefings\n",
    "# check next page\n",
    "\n",
    "#check if there's a 'next' button and store associated url\n",
    "next_url = current_page_briefings.find(\"a\",{\"next page-numbers\"})['href']\n",
    "\n",
    "# if so, collect the subsequent urls to scrape\n",
    "if next_url:\n",
    "    page_url_list = [all_briefings_resp.url]\n",
    "    while next_url:\n",
    "        page_url_list.append(next_url)\n",
    "        current_page_briefings = bs(requests.get(next_url).text)\n",
    "        time.sleep(.4)\n",
    "        try:\n",
    "            next_url = current_page_briefings.find(\"a\",{\"next page-numbers\"})['href']\n",
    "        except:\n",
    "            next_url = None\n",
    "            \n",
    "transcript_urls = []\n",
    "for url in page_url_list:\n",
    "    page_briefings = bs(requests.get(url).text)\n",
    "    page_briefings_list = page_briefings.find_all(\"div\",{\"class\":\"fl-post-column\"})\n",
    "    time.sleep(.5)\n",
    "    for p in page_briefings_list:\n",
    "        transcript_urls.append(p.find(\"a\")['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_briefing_dfs = []\n",
    "\n",
    "for url in transcript_urls:\n",
    "    briefing = bs(requests.get(url).text)\n",
    "    time.sleep(.4)\n",
    "    \n",
    "    briefing_text = briefing.find(\"div\",{\"class\":\"fl-callout-text\"})\n",
    "    \n",
    "    # empty lists to store transcript columns\n",
    "    speaker = []\n",
    "    timestamp = []\n",
    "    text = []\n",
    "\n",
    "    # iterate over transcript paragraphs\n",
    "    for paragraph in briefing_text.find_all(\"p\"):\n",
    "\n",
    "        name_match = re.search('(.+?):', paragraph.text)\n",
    "        if name_match:\n",
    "            speaker.append(name_match.group(1))\n",
    "\n",
    "        timestamp_match = re.search('\\((.+?)\\)', paragraph.text)\n",
    "        if timestamp_match:\n",
    "            timestamp.append(timestamp_match.group(1))\n",
    "\n",
    "        corp_match_index = paragraph.text.find(\")\")+2\n",
    "        if corp_match_index:\n",
    "            text.append(paragraph.text[corp_match_index:])\n",
    "\n",
    "    # combine lists into a single df\n",
    "    briefing_data = pd.DataFrame(list(zip(timestamp, speaker, text)),\n",
    "                                     columns = ['timestamp','speaker','text'])\n",
    "\n",
    "    # extract briefing date and add to df\n",
    "    date_string = briefing.find_all(\"div\",{\"class\":\"fl-rich-text\"})[0].text\n",
    "    if date_string:\n",
    "        briefing_date = datetime.strptime(date_string, '%b %d, %Y')\n",
    "\n",
    "    briefing_data.insert(0,'date',briefing_date)\n",
    "    \n",
    "    # export single briefing as csv\n",
    "    filename = \"../data/whtfb/\" + briefing_date.strftime(\"%Y-%m-%d\")+(\"_tfb.csv\")\n",
    "    briefing_data.to_csv(filename,index=False)   \n",
    "    \n",
    "    # store all briefings in a single df\n",
    "    all_briefing_dfs.append(briefing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_briefings = pd.concat(reversed(all_briefing_dfs))\n",
    "merged_briefings.to_csv(\"../data/whtfb/all_briefings.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
